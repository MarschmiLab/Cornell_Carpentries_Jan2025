---
title: "R for Data Analysis"
source: Rmd
teaching: 150
exercises: 15
questions:
- "How can I summarize my data in R?"
- "How can R help make my research more reproducible?"
- "How can I combine two datasets from different sources?"
- "How can data tidying facilitate answering analysis questions?"
objectives:
- "To become familiar with the functions of the `dplyr` and `tidyr` packages."
- "To be able to use `dplyr` and `tidyr` to prepare data for analysis."
- "To be able to combine two different data sources using joins."
- "To be able to create plots and summary tables to answer analysis questions."
keypoints:
- "Package loading is an important first step in preparing an R environment."
- "Data analsyis in R facilitates reproducible research."
- "There are many useful functions in the `tidyverse` packages that can aid in data analysis."
- "Assessing data source and structure is an important first step in analysis."
- "Preparing data for analysis can take significant effort and planning."
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("04-")
options(pillar.width = 350,
        width = 350)
```

### Contents

1.  [Getting started](#getting-started)
    -   [Loading in the data](#loading-in-the-data)
2.  [An introduction to data analysis in R using `dplyr`](#intro-data-analysis)
    -   [Get stats fast with `summarize()`](#get-stats-fast-with-summarize)
    -   [Narrow down rows with `filter()`](#narrow-down-rows-with-filter)
    -   [Grouping rows using `group_by()`](#grouping-rows-using-group_by)
    -   [Make new variables with `mutate()`](#make-new-variables-with-mutate)
    -   [Subset columns using `select()`](#subset-columns-using-select)
    -   [Changing the shape of the data](#changing-the-shape-of-the-data)
3.  [Cleaning up data](#cleaning-up-data)
4.  [Joining data frames](#joining-data-frames)
5.  [Analyzing combined data](#analyzing-combined-data)
6.  [Finishing with Git and GitHub](#Finishing-with-Git-and-GitHub)

# Getting Started

First, navigate to the ontario-reports directory however you'd like and open `ontario-report.Rproj`.
This should open the ontario-report R project in RStudio.
You can check this by seeing if the Files in the bottom right of RStudio are the ones in your `ontario-report` directory.

Yesterday we spent a lot of time making plots in R using the ggplot2 package. Visualizing data using plots is a very powerful skill in R, but what if we would like to work with only a subset of our data? Or clean up messy data, calculate summary statistics, create a new variable, or join two datasets together? There are several different methods for doing this in R, and we will touch on a few today using functions the `dplyr` package.

First, we will create a new RScript file for our work. Open RStudio. Choose "File" \> "New File" \> "RScript". We will save this file as `data_analysis.R`

### Loading in the data

We will start by importing the complete sample dataset that we used yesterday into our fresh new R session. Today let's type them into the console ourselves: `sample_data <- read_csv("data/sample_data.csv")`

> ## Exercise
>
> If we look in the console now, we'll see we've received an error message saying that R "could not find the function `read_csv()`". *Hint: Packages...*
>
> > ## Solution
> >
> > What this means is that R cannot find the function we are trying to call. The reason for this usually is that we are trying to run a function from a package that we have not yet loaded. This is a very common error message that you will probably see a lot when using R. It's important to remember that you will need to load any packages you want to use into R each time you start a new session. The `read_csv` function comes from the `readr` package which is included in the `tidyverse` package so we will just load the `tidyverse` package and run the import code again.
> {: .solution}
{: .challenge}

Now that we know what's wrong, We will use the `read_csv()` function from the Tidyverse `readr` package. Load the `tidyverse` package and sample dataset using the code below.

```{r InitDplyr}
library(tidyverse)
```

The output in your console shows that by doing this, we attach several useful packages for data wrangling, including `readr`. Check out these packages and their documentation at [tidyverse.org](https://www.tidyverse.org)

> **Reminder:** Many of these packages, including `dplyr` , come with "Cheatsheets" found under the **Help** RStudio menu tab.

Reload your data:

```{r Reloadsample}
sample_data <- read_csv("data/sample_data.csv")
```

Notice that the output of the `read_csv()` function is pretty informative. It tells us the name of all of our column headers as well as how it interpreted the data type. This birds-eye-view can help you take a quick look that everything is how we expect it to be.

Now we have the tools necessary to work through this lesson.

# An introduction to data analysis in R using `dplyr` {#intro-data-analysis}

## Get stats fast with `summarize()` {#get-stats-fast-with-summarize}

[*Back to top*](#contents)

Let's say we would like to know what is the mean (average) cell abundance in our dataset. R has a built in function function called `mean()` that will calculate this value for us. We can apply that function to our cells_per_ml column using the `summarize()` function. Here's what that looks like:
```{r Avgcells_per_ml}
summarize(sample_data, average_cells=mean(cells_per_ml))
```

When we call `summarize()`, we can use any of the column names of our data object as values to pass to other functions. `summarize()` will return a new data object and our value will be returned as a column.

> **Note:** The `summarize()` and `summarise()` perform identical functions.

We name this new column so we can use in a future argument. So the `average_cells=` part tells `summarize()` to use "average_cells" as the name of the new column. Note that you don't have to quotes around this new name as long as it starts with a letter and doesn't include a space.

Instead of including the data as an argument, we can use the *pipe operator* `%>%` to pass the data value into the `summarize` function. 

```{r Avgcells_per_mlWithPipe}
sample_data %>% summarize(average_cells=mean(cells_per_ml))
```
 
This line of code will do the exact same thing as our first summary command, but the piping function tells R to use the `sample_data` dataframe as the first argument in the next function.

This lets us "chain" together multiple functions, which will be helpful later. Note that the pipe (`%>%`) is a bit different from using the ggplot plus (`+`). Pipes take the output from the left side and use it as input to the right side. Plusses layer on additional information (right side) to a preexisting plot (left side). 
 
We can also add an <kdb>Enter</kdb> to make it look nicer:  
```{r Avgcells_per_mlWithPipe2}
sample_data %>%
  summarize(average_cells=mean(cells_per_ml))
```

Using the *pipe operator* `%>%` and enter command makes our code more readable. The  *pipe operator* `%>%` also helps to avoid using nested functions and minimizes the need for new variables.

Since we use the pipe operator so often, there is a keyboard shortcut for it in RStudio. You can press <kdb>Ctrl</kdb>+<kdb>Shift</kdb>+<kdb>M<kdb> on Windows or <kdb>Cmd<kdb>+<kdb>Shift<kdb>+<kdb>M<kdb> on a Mac.

> ## Pro tip: Saving a new dataframe
> Notice that when we run the following code, we are not actually saving a new variable: 
> ```{r Avgcells_per_mlWithPipe3, eval = FALSE}
> sample_data %>%
>   summarize(average_cells=mean(cells_per_ml))
> ```
> This simply outputs what we have created, but does not change actually change `sample_data` or save a new dataframe. To save a new dataframe, we could run: 
> ```{r saveNewDF, eval = FALSE}
> sample_data_summarized <- sample_data %>%
>   summarize(average_cells=mean(cells_per_ml))
> ```
> Or if we want to change `sample_data` itself: 
> ```{r overwriteDF, eval = FALSE}
> sample_data <- sample_data %>%
>   summarize(average_cells=mean(cells_per_ml))
> ```
> IMPORTANT: This would overwrite the existing `sample_data` object. Let's not do this!
> 
> For now, we will not be saving dataframes, since we are just experimenting with `dyplr` functions, but it will be useful later on in this lesson. 
> 
{: .callout}

## Narrow down rows with `filter()` {#narrow-down-rows-with-filter}

[*Back to top*](#contents)

Let's take a look at the value we just calculated, which tells us the average cell abundance for all rows in the data was 4,046,023. However, we previously saw that cell abundance varied widely between our environmental groups. It may be more informative to calculate average cell abundance for each of those groups separately. 

Let's start by just calculating cell abundance within the Deep group. To do that, we will use the `filter()` function to only use rows for that group before calculating the mean value.

```{r GapRecentcells_per_ml}
sample_data %>%
  filter(env_group == "Deep") %>%
  summarize(average=mean(cells_per_ml))
```

> ## Filtering the dataset
>
> What is the average chlorophyll concentration in Deep samples? *Hint: the column headers identified by `read_csv()` showed us there was a column called chlorophyll in the dataset*
>
> > ## Solution
> >
> > ```{r FilterChallenge1}
> > sample_data %>%
> > filter(env_group == "Deep") %>%
> > summarize(average_chl=mean(chlorophyll))
> > ```
> > {: .source}
> > By combining `filter()` and `summarize()` we were able to calculate the mean chlorophyll in the Deep samples.
> {: .solution}
>  What about in shallow September?
>
> > ## Solution
> >
> > ```{r FilterChallenge2}
> > sample_data %>%
> > filter(env_group == "Shallow_September") %>%
> > summarize(average_chl=mean(chlorophyll))
> > ```
> > {: .source}
> > Wow - much higher than in the deep samples!.
> {: .solution}
{: .challenge}

Notice how the pipe operator (`%>%`) allows us to combine these two simple steps into a more complicated data extraction?. We took the data, filtered out the rows, then took the mean value. The argument we pass to `filter()` needs to be some expression that will return TRUE or FALSE. We can use comparisons like `>` (greater than) and `<` (less than) for example. Here we tested for equality using a double equals sign `==`. You can also use `!=` to mean "doesn't equal". You use `==` (double equals) when testing if two values are equal, and you use `=` (single equals) when naming arguments that you are passing to functions. Try changing it to use `filter(env_group = "Deep")` and see what happens.

## Grouping rows using `group_by()` {#grouping-rows-using-group_by}

[*Back to top*](#contents)

We see that the cell abundance in Deep samples is much lower than the value we got using all of the rows. We saw that by using filtering, we can calculate statistics for each group separately, but we need to do this one at a time. Rather that doing a bunch of different `filter()` statements, we can instead use the `group_by()` function. The function allows us to tell the code to treat the rows in logical groups, so rather than summarizing over all the rows, we will get one summary value for each group. Here's what that will look like:

```{r Gapcells_per_mlByYear}
sample_data %>%
  group_by(env_group) %>%
  summarize(average=mean(cells_per_ml))
```

The `group_by()` function expects you to pass in the name of a column (or multiple columns separated by comma) in your data. 

Note that you might get a message about the summarize function regrouping the output by 'env_group'. This simply indicates what the function is grouping by. 

> ## Grouping the data
>
> Try calculating the average temperature by env_group.
>
> > ## Solution
> >
> > ```{r GroupByChallenge1}
> > sample_data %>%
> > group_by(env_group) %>%
> > summarize(average=mean(temperature))
> > ```
> > {: .source}
> >
> > By combining `group_by()` and `summarize()` we are able to calculate the mean temperature by environmental group.
> {: .solution}
{: .challenge}

You can also create more than one new column when you call `summarize()`. To do so, you must separate your columns with a comma. Building on the code from the last exercise, let's add a new column that calculates the minimum cell abundance for each env_group. 

```{r Gapcells_per_mlMinMax}
sample_data %>%
  group_by(env_group) %>%
  summarize(average=mean(cells_per_ml), min=min(cells_per_ml))
```


## Make new variables with `mutate()` {#make-new-variables-with-mutate}

[*Back to top*](#contents)

Each time we ran `summarize()`, we got back fewer rows than passed in. We either got one row back, or one row per group. But sometimes we want to create a new column in our data without changing the number of rows. The function we use to create new columns is called `mutate()`.

We have a column for the total nitrogen and the total phosphorus. Often, we also want to report the ratio between these two nutrients (researchers often report this as a molar ratio, but for simplicity we'll just use their masses). Here's what such a `mutate()` command would look like:

```{r GapMutate}
sample_data %>%
  mutate(tn_tp_ratio = total_nitrogen / total_phosphorus)
```

This will add a new column called "tn_tp_ratio" to our data. We use the column names as if they were regular values that we want to perform mathematical operations on and provide the name in front of an equals sign like we have done with `summarize()`

> ## `mutate()`
> We can also multiply by constants or other numbers using mutate - remember how in the plotting lesson we made a plot with cells_per_ml in millions? Try making a new column for this dataframe called cellsInMillions that is the cell abundance in millions. 
> 
> > ## Solution: 
> > ```{r mutateExercise}  
> > sample_data %>%  
> > mutate(tn_tp_ratio = total_nitrogen / total_phosphorus, 
> >        cellsInMillions = cells_per_ml / 1000000)  
> > ```  
> {: .solution}
{: .challenge}
 

## Subset columns using `select()` {#subset-columns-using-select}

[*Back to top*](#contents)

We use the `filter()` function to choose a subset of the rows from our data, but when we want to choose a subset of columns from our data we use `select()`. For example, if we only wanted to see the sample IDs ("sample_id") and depth values, we can do:

```{r GapSelect}
sample_data %>%
  select(sample_id, depth)
```

We can also use `select()` to drop/remove particular columns by putting a minus sign (`-`) in front of the column name. For example, if we want everything but the env_group column, we can do:

```{r GapSelectDrop}
sample_data %>%
  select(-env_group)
```

> ## selecting columns 
> Create a dataframe with only the `sample_id`, `env_group`, `depth`, `temperature`, and `cells_per_ml` columns. 
> 
> > ## Solution: 
> > There are multiple ways to do this exercise. Here are two different possibilities. 
> > 
> > ```{r selectExerciseOption1}
> > sample_data %>%
> >   select(sample_id, env_group, depth, temperature, cells_per_ml)
> > ```
> > ```{r selectExerciseOption2}
> > sample_data %>%
> >   select(-total_nitrogen, -total_phosphorus, -diss_org_carbon, -chlorophyll)
> > ```
> > Did you notice that the order of columns also changed, to match the order of arguments in the select function?
> {: .solution}
{: .challenge}

> ## Pro tip: Selecting a range of columns
> Sometimes, we want to select a range of contiguous columns. Rather than writing out each column name, we can use the `:` operator to selecting a range of columns
> ```{r rangeofColumns, eval = FALSE}
> sample_data %>%
>   select(sample_id:temperature)
> ```
> We can also drop a range of columns in this way, though we need to include some extra parentheses:
> ```{r rangeofColumns2, eval = FALSE}
> sample_data %>%
>   select(-(total_nitrogen:chlorophyll))
> ```
{: .callout}

> ## Bonus: Using helper functions with `select()`
>
> The `select()` function has a bunch of helper functions that are handy if you are working with a dataset that has a lot of columns. You can see these helper functions on the `?select` help page. For example, let's say we wanted to select the sample_id column and all the columns that start with "total". You can do that with:
> 
> ```{r GapSelectFancy}
> sample_data %>%
>   select(sample_id, starts_with("total"))
> ```
> This returns just the three columns we are interested in. 
>
> > ## Using `select()` with a helper function
> >
> > Find a helper function on the help page that will choose all the columns that have "n" as their last letter
> >
> > > ## Solution
> > >
> > > The helper function `ends_with()` can help us here.
> > >
> > > ```{r GroupByChallenge2}
> > > sample_data %>%
> > > select(ends_with("n"))
> > > ```
> > {: .solution}
> {: .challenge}
> 
{: .solution}

# Cleaning up data

[*Back to top*](#contents)

Researchers are often pulling data from several sources, and the process of making data compatible with one another and prepared for analysis can be a large undertaking. Luckily, there are many functions that allow us to do this in R. We've been working with the sample dataset, which contains metadata about each sample we collected. In this section, we practice cleaning and preparing a second dataset derived from sequencing data, which holds the relative abundances of the most abundant Phyla in each sample.

It's always good to go into data cleaning with a clear goal in mind. Here, we'd like to prepare the taxon data to be compatible with our sample data so we can directly compare the abundance of specific Phyla to our sample data. To make this work, we'd like a data frame that contains a column with the sample ID, and columns for the abundances of each Phyla. Let's start with reading the data in using `read_csv()`

```{r CleanEmissionsData}
read_csv("data/taxon_abundance.csv")
```

Hmmm, this looks a little weird. It is telling us that our table has 74 rows and just 1 column. Looking at the table that is outputted by `read_csv()` we can see that there appear to be two rows at the top of the file that contain extraneous information about the data in the table. Ideally, we'd skip those. We can do this using the `skip=` argument in read_csv by giving it a number of lines to skip.

```{r CleanEmissionsSkip}
read_csv("data/taxon_abundance.csv", skip=2)
```

This is working better - we now have 72 rows and 13 columns. However, we're also getting a message that `read_csv` has made a new column name. This can happen if your file has data in rows which lack a column name. In that case, `dplyr` creates new column names, which start with "...". Here, it looks like the final column contains information about the sequencing machine which generated the data. 

> ## Warnings and Errors
> It's important to differentiate between Warnings and Errors in R. A warning tells us, "you might want to know about this issue, but R still did what you asked". An error tells us, "there's something wrong with your code or your data and R didn't do what you asked". You need to fix any errors that arise. Warnings, are probably best to resolve or at least understand why they are coming up.
{.callout}

Perhaps we should rename this column to something informative. We can use the `rename` function to do so. While we're at it, let's save this as an object which we can work with down the line.

```{r CleanEmissionsColName}
taxon_dirty <- read_csv("data/taxon_abundance.csv", skip=2) %>%
    rename(sequencer = ...13)

glimpse(taxon_dirty)
```


> ## Bonus: Modifying multiple column names at once
> 
> Many data analysts prefer to have their column headings and variable names be in all lower case. We can use a variation of `rename()`, which is `rename_all()` that allows us to set all of the column headings to lower case by giving it the name of the tolower function, which makes everything lowercase.
> 
> ```{r CleanEmissionsRenameLower}
> read_csv("data/taxon_abundance.csv", skip=2) %>%
>  rename_all(tolower)
> ```
{: .solution}

We previously saw how we can subset columns from a data frame using the select function. There are a lot of columns with extraneous information in this dataset. For instance, while information like the number of sequencing reads, the DNA extraction date, and the lot_number might be useful to use in the lab, it won't be a part of our analysis. Let's subset out the columns we are interested in, which includes the sample IDs and taxon information (columns from Proteobacteria to Planctomycetota).

> ## Reviewing selecting columns
> Select the columns of interest from our dataset. Note, there are multiple ways to do this.
> 
> > ## Solution: 
> > ```{r CleanEmissionsSelect}
> > taxon_dirty %>%
> >   select(sample_id, Proteobacteria:Planctomycetota)
> > ```
> > ```{r CleanEmissionsSelect2}
> > taxon_dirty %>%
> >   select(-(sequencing_reads:sequencer))
> > ```
> > ```{r CleanEmissionsSelect3}
> > taxon_dirty %>%
> >   select(sample_id, Proteobacteria, Actinobacteriota, Bacteroidota, Chloroflexi, Verrucomicrobiota, Cyanobacteria, Planctomycetota)
> > ```
> {: .solution}
{: .challenge}

Let's go ahead and assign the output of this code chunk, which is the cleaned dataframe, to a variable name:

```{r AssignCleanEmissions}
taxon_clean <- taxon_dirty %>% 
  select(sample_id, Proteobacteria:Planctomycetota)
```

> **Looking at your data:** You can get a look at your data-cleaning hard work by navigating to the **Environment** tab in RStudio and clicking the table icon next to the variable name. Notice when we do this, RStudio automatically runs the `View()` command. We've made a lot of progress!
{.callout}

## Changing the shape of the data

[*Back to top*](#contents)

Data comes in many shapes and sizes, and one way we classify data is either "wide" or "long." Data that is "long" has one row per observation, and each column represents a unique, independent variable. The sample_data data is in a long format. 

In wide data, each row represents a group of observations and each value is placed in a different column rather than a different row. Our taxon data is in wide format, as taxa abundance is spread out across multiple columns which represent different species. To be honest, the lines between "wide" and "long" can be blurry. One clue that your data mind be wide is if you have multiple columns which all share the same units and scale, or add up to a larger whole. In our taxon abundance table, values in each row correspond to a fraction of that samples "whole" (100% of its composition).

The `tidyr` package contains the functions `pivot_wider` and `pivot_longer` that make it easy to switch between the two formats. The `tidyr` package is included in the `tidyverse` package so we don't need to do anything to load it. Let's convert out taxon_clean to long format. 

```{r PivotLonger}
taxon_long <- taxon_clean %>%
    pivot_longer(cols = Proteobacteria:Planctomycetota, 
                 names_to = "Phylum",
                 values_to = "Abundance")

taxon_long
```

Notice how much longer our table is. We might describe this data as "tidy" because it makes it easy to work with `ggplot2` and `dplyr` functions (this is where the "tidy" in "tidyverse" comes from). For example, this allows us to use `group_by` to calculate the average relative abundance for each Phylum:

```{r groupPhylum}
taxon_long %>%
    group_by(Phylum) %>%
    summarize(avg_abund = mean(Abundance))
```

Long format is also, at times, necessary for specific types of plotting. For example, let's make a very common plot in microbial ecology: the stacked bar plot. Notice how we are piping our dataframe into our ggplot call.

```{r stackedBars}
taxon_long %>%
    ggplot(aes(x = sample_id, y = Abundance, fill = Phylum)) + 
    geom_col() + 
    theme(axis.text.x = element_text(angle = 90))

```

Our bars don't add up perfectly to one, as our data doesn't include rarer Phyla. 

If we want to return our data to wide format, we can use `pivot_wider`

```{r pivotWider}
taxon_long %>%
    pivot_wider(names_from = "Phylum", values_from = "Abundance")
```

# Joining data frames

[*Back to top*](#contents)


Now we're ready to join our taxon data to the sample data. Previously we saw that we could read in and filter the sample data like this to get the data from the Americas for 2007 so we can create a new dataframe with our filtered data:

```{r Loadsample2007}
head(sample_data, 6)
head(taxon_clean, 6)
```

Look at the data in `taxon_clean` and `sample_data`. If you had to merge these two data frames together, which column would you use to merge them together? If you said "sample_id" - good job!

We'll call sample_id our "key". Now, when we join them together, can you think of any problems we might run into when we merge things? We might not have taxon data for all of the countries in the sample dataset and vice versa. 

The dplyr package has a number of tools for joining data frames together depending on what we want to do with the rows of the data that are not represented in both data frames. Here we'll be using `inner_join()` and `anti_join()`. 

In an "inner join", the new data frame only has those rows where the same key is found in both data frames. This is a very commonly used join.

![]({{ page.root }}/fig/r-data-analysis/join-inner.png)

> ## Bonus: Other dplyr join functions 
>
> Outer joins and can be performed using `left_join()`, `right_join()`, and `full_join()`. In a "left join", if the key is present in the left hand data frame, it will appear in the output, even if it is not found in the the right hand data frame. For a right join, the opposite is true. For a full join, all possible keys are included in the output data frame.
> 
> ![]({{ page.root }}/fig/r-data-analysis/join-outer.png)
{: .solution}

Let's give the `inner_join()` function a try.
```{r InnerJoin}
inner_join(sample_data, taxon_clean)
```

Do you see that we now have data from both data frames joined together in the same data frame? One thing to note about the output is that `inner_join()` tells us that that it joined by "sample_id". We can make this explicit using the "by" argument in the join functions

```{r InnerJoinByCountry}
inner_join(sample_data, taxon_clean, by="sample_id")
```

One thing to notice is that sample data had 71 rows, but the output of our join only had 32. Let's investigate. It appears that there must have been samples in the sample data that did not appear in our taxon_clean data frame. 

Let's use `anti_join()` for this - this will show us the data for the keys on the left that are missing from the data frame on the right. 

```{r AntiJoin}
anti_join(sample_data, taxon_clean, by="sample_id")
```

We can see that none of the joining worked for September samples! What's going on here? Well, let's look closer at those sample IDs. In our sample_data, our sample IDs used the full word "September", but in our taxon table it looks like someone shortened it to "Sep". As such, `inner_join` can recognize that these are matching keys. Let's use what we know about `mutate` alongside a new function, `str_replace`, to help them match. 

```{r CleanEmissionsMutateRecode}
taxon_clean_goodSept <- taxon_clean %>%
  mutate(sample_id = str_replace(sample_id, pattern = "Sep", replacement = "September"))
```

Now let's try with that inner join again.

```{r final-innerJoin}
inner_join(sample_data, taxon_clean_goodSept, by = "sample_id")
```

Woohoo! We matched all of the samples from each table. Let's save this as a new dataframe, which we can use in later projects. 

```{r save_joined}
sample_and_taxon <- inner_join(sample_data, taxon_clean_goodSept, by = "sample_id")
```

We have reached our data cleaning goals! One of the best aspects of doing all of these steps coded in R is that our efforts are reproducible, and the raw data is maintained. With good documentation of data cleaning and analysis steps, we could easily share our work with another researcher who would be able to repeat what we've done. However, it's also nice to have a saved `csv` copy of our clean data. That way we can access it later without needing to redo our data cleaning, and we can also share the cleaned data with collaborators. To save our dataframe, we'll use `write_csv()`. 

```{r writeCSV}
write_csv(sample_and_taxon, "data/sample_and_taxon.csv")
```

Great - Now we can move on to the analysis! 

# Analyzing combined data

[*Back to top*](#contents)

Now comes the fun part! All this cleaning and joining has given us a dataset which allows us to look at how environmental variables affects the abundance of different microbial Phyla. For this portion, we are going to focus on the Phylum Chloroflexi. We want to figure out - where do species in this Phylum tend to live in Lake Ontario? Are there differences in its abundance between environmental groups?

To answer the first question, we'll plot the relative abundance of Chloroflexi against a station's depth using a scatter plot:

```{r PlotPercapCO2vsGDP}
ggplot(sample_and_taxon, aes(x=depth, y=Chloroflexi)) +
  geom_point()+
  labs(x="Depth (m)",
       y="Chloroflexi (Relative Abundance)",
       title="There is a strong association between a station's depth \nand the abundance of Chloroflexi"
  )
```

*Tip:* Notice we used the `\n` in our title to get a new line to prevent it from getting cut off.

To help clarify the association, we can add a fit line through the data using `geom_smooth()`

```{r PlotPercapCO2vsGDPSmooth}
ggplot(sample_and_taxon, aes(x=depth, y=Chloroflexi)) +
  geom_point()+
  labs(x="Depth (m)",
       y="Chloroflexi (Relative Abundance)",
       title="There is a strong association between a station's depth \nand the abundance of Chloroflexi"
  ) +
  geom_smooth()
```

By default, ggplot uses a funky model called "loess estimation" We can force the line to be a linear model (straight) using `method="lm"` as an argument to `geom_smooth`

```{r PlotPercapCO2vsGDP1SmoothLm}
ggplot(sample_and_taxon, aes(x=depth, y=Chloroflexi)) +
  geom_point()+
  labs(x="Depth (m)",
       y="Chloroflexi (Relative Abundance)",
       title="There is a strong association between a station's depth \nand the abundance of Chloroflexi"
  ) +
  geom_smooth(method="lm")
```

To answer our first question, Chloroflexi appears to be much more abundant in the deeper samples from Lake Ontario!

For the second question, we want to calculate the average abundance of Chloroflexi in our different environmental groups, using a combination of `group_by` and `summarize`. Let's also calculate the standard deviation, to give us a sense of the variance in each group.

```{r GapRegionGroupSummary}
sample_and_taxon %>%
  group_by(env_group) %>%
  summarize(avg_chloro = mean(Chloroflexi),
            sd_chloro = sd(Chloroflexi))
```

We see that Chloroflexi is most abundant in the Deep groups, relatively rare in Shallow May, and very rare in Shallow September. 

# Bonus 

## Bonus content

### Sort data with `arrange()`

The `arrange()` function allows us to sort our data by some value. Let's use the `sample_data` dataframe. We will take the average value for each environmental group and then sort it so the environmental group with the highest cell abundance are on top. Which environmental group might you guess has the highest cell abundance before running the code?

```{r GapLifeContinentArrange}
sample_data %>%
 group_by(env_group) %>%
 summarise(average= mean(cells_per_ml)) %>%
 arrange(desc(average))
```

Notice there that we can use the column created the in the `summarize()` step ("average") later in the `arrange()` step. We also use the `desc()` function here to sort the values in a descending order so the largest values are on top. The default is to put the smallest values on top.


## Bonus exercises

> ## Calculating absolute abundance
>
> Our data contains total cell abundance (`cells_per_ml`) alongside the relative abundance of each Phylum. If we wanted to get the absolute abundance of Chloroflexi (Cells of Chloroflexi per ml), how would we calculate that?
>
> > ## Solution
> >
> > Create a new variable using `mutate()` that calculates absolute abundance of Chloroflexi.
> >
> > ```{r PopPercExercise}
> > sample_and_taxon %>%
> >   mutate(abs_Chloroflexi = cells_per_ml * Chloroflexi) %>%
> >   glimpse()
> > ```
> >
> > Previously, we found that Deep samples had the greatest *relative* abundance of Chloroflexi. Do they also have the greatest *absolute* abundance of this taxa?
> >
> > ```{r}
> > sample_and_taxon %>%
> >   mutate(abs_Chloroflexi = cells_per_ml * Chloroflexi) %>%
> >   group_by(env_group) %>%
> >   summarize(avg_chloro_relative = mean(Chloroflexi),
> >             avg_chloro_absolute = mean(abs_Chloroflexi))
> > ```
> > Looks like yes! However, the difference is much more slight between Deep and Shallow_May when we consider absolute abundance.
> {: .solution}
{: .challenge}
